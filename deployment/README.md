# Deployment

* [AWS Credentials](#aws-credentials)
* [Publish Container Images](#publish-container-images)
* [Terraform](#terraform)

## AWS Credentials

Using the AWS CLI, create an AWS profile named `open-apparel-registry`:

```bash
$ aws configure --profile open-apparel-registry
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: eu-west-1
Default output format [None]:
```

You will be prompted to enter your AWS credentials, along with a default region. These credentials will be used to authenticate calls to the AWS API when using Terraform and the AWS CLI.


## Publish Container Images

Before we can deploy this project's core infrastructure, we will need to build a container image and publish it somewhere accessible to Amazon's services.

AWS Elastic Container Registry (ECR) is a good candidate because ECR authentication with AWS Elastic Container Service (ECS) is handled transparently. If we wanted to use private images hosted on Quay, for example, we'd have to make [changes](https://docs.quay.io/issues/ecs-auth-failure.html) to configuration files on the EC2 Container Instances.

To do this, we can use the `cibuild` and `cipublish` scripts:

```bash
$ vagrant ssh
vagrant@vagrant:/vagrant$ export OAR_AWS_ECR_ENDPOINT=249322298638.dkr.ecr.eu-west-1.amazonaws.com
vagrant@vagrant:/vagrant$ ./scripts/cibuild
...
Successfully built 20dcf93f6907
Successfully tagged openapparelregistry:a476b78
vagrant@vagrant:/vagrant$ ./scripts/cipublish
...
```

## Terraform

First, we need to make sure there is a `terraform.tfvars` file in the project settings bucket on S3. The `.tfvars` file is where we can change specific attributes of the project's infrastructure, not defined in the `variables.tf` file.

Here is an example `terraform.tfvars` for this project:

```hcl
project = "OpenApparelRegistry"
environment = "Staging"

aws_key_name = "oar-stg"

r53_private_hosted_zone = "osh.internal"
r53_public_hosted_zone = "oshstaging.openapparel.org"

cloudfront_price_class = "PriceClass_100"

external_access_cidr_block = ""

bastion_ami = "ami-0ff8a91507f77f867"
bastion_instance_type = "t3.nano"

google_server_side_api_key = ""
google_client_side_api_key = ""
google_analytics_key = ""

rollbar_server_side_access_token = ""
rollbar_client_side_access_token = ""

django_secret_key = "secret"

rds_database_identifier = "openapparelregistry"
rds_database_name = "openapparelregistry"
rds_database_username = "openapparelregistry"
rds_database_password = "password"

oar_client_key = ""
```

This file lives at `s3://opensupplyhub-staging-config-eu-west-1/terraform/terraform.tfvars`.

To deploy this project's core infrastructure, use the `infra` wrapper script to lookup the remote state of the infrastructure and assemble a plan for work to be done:

```bash
vagrant@vagrant:/vagrant$ docker-compose -f docker-compose.ci.yml run --rm terraform
bash-4.4# export GIT_COMMIT=a476b78
bash-4.4# ./scripts/infra plan
```

Once the plan has been assembled, and you agree with the changes, apply it:

```bash
bash-4.4# ./scripts/infra apply
```

This will attempt to apply the plan assembled in the previous step using Amazon's APIs.

## Logs and Debugging

### RequestLogs

The RequestLog table in the OAR database, accessible via the Django Admin, is used to count and report on requests made with an API token.

### CloudWatch logs and Dashboard

Navigate to [CloudWatch](https://eu-west-1.console.aws.amazon.com/cloudwatch/home?region=eu-west-1#dashboards:). Select the dashboard for the environment you're investigating. Adjust the dashboard to reflect the time period you're interested in using the 'Custom' time filter, if needed.

The Request Latency graph shows the top 50% fastest response times, the top 90%, and the top 99%. Requests that take over 30 seconds to respond will time out.

The Requests graph shows the number of requests being received at a certain time.

The Database CPU graph shows the percentage of database CPU being used at a certain time.

CloudWatch logs are divided into groups. You will most likely be interested in the ones named log[Environment]App and log[Environment]AppCLI. Logs with the App suffix are from web tasks, and logs with the AppCLI suffix were generated by running Django management commands.

CloudWatch also contains Log Insights, which allows you to query the CloudWatch logs (sample queries are available in AWS).

### RDS

Visit [RDS](https://eu-west-1.console.aws.amazon.com/rds/home?region=eu-west-1) in AWS.

Navigate to [Databases](https://eu-west-1.console.aws.amazon.com/rds/home?region=eu-west-1#databases), then select the database you're investigating. Scroll down and navigate to 'Logs & events'.

Identify the log(s) for the time period of interest (sorting by 'last written' will help). Note that the 'Last written' time displayed will be in your local time. Download the log or logs.

Logs can also be downloaded via the AWS CLI, which can be less cumbersome if you need to download a range of 1-hour log segments and can be automated. Here is an example command.

```
AWS_PROFILE=open-apparel-registry aws rds download-db-log-file-portion --db-instance-identifier  openapparelregistry-enc-prd  --log-file-name error/postgresql.log.2022-09-13-17 > postgresql.log.2022-09-13-17.txt
```

Install [pgBadger](https://pgbadger.darold.net/). In the terminal, enter the command `pgbadger` followed by one or more filepaths to the logfiles you downloaded. For example (with multiple filepaths):

```
pgbadger ~/Desktop/postgresql.log.2022-09-26-16 ~/Desktop/postgresql.log.2022-09-26-15
```

This will generate a file called `out.html` in your working directory. Open this file in your browser to see an analysis of queries during the times covered by your logs. Note that the times in the analysis will be in GMT.

### Athena

Visit [Athena](https://eu-west-1.console.aws.amazon.com/athena/home?region=eu-west-1#/query-editor). Here you can run queries on the staging and production logs.

We collect request logs from both CloudFront, the CDN, and the Application Load Balancer. There is a delay of at least several minutes between when requests are made and when they are queryable in Athena.

The services are arranged like so: CloudFront -> Aplication Load Balancer -> Elastic Container Service Cluster

A few example queries are below, to give you an idea of how this resource can be used.

A query to find the 100 requests that took the longest to respond since 2022-09-26 14:00 (replace with a time shortly before the latency spike you're investigating).
```
SELECT *
FROM "default"."prd_alb_logs"
WHERE time > '2022-09-26T14:00'
order by target_processing_time desc
limit 100
```

A query to find the 1000 most recent CDN logs.
```
SELECT *
FROM "default"."prd_cdn_logs"
order by date desc, time desc
limit 1000
```

Find the most frequent request IPs for a period (has been used to identify spikes in requests from specific countries).
```
SELECT count(*) AS num, request_ip
FROM "default"."prd_cloudfront_logs"
WHERE date = date '2022-09-26' AND time > '14:00' AND time < '15:00'
GROUP BY request_ip
ORDER BY num DESC;
```
